{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demo:  Computing Gradients\n",
    "\n",
    "Most numerical optimization methods require that we compute gradients of the loss function that we are attempting to minimize.  In this demo, we illustrate how to compute gradients efficiently in python for a few simple examples.  As much as possible, we avoid for loops for fast implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 1:  A Simple Vector-Input Function\n",
    "\n",
    "Suppose `f(w) = w_0^2 + 2w_0w_1^3`.  Then the function below computes `f(w)` its gradient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def feval(w):\n",
    "\n",
    "    # Function\n",
    "    f = w[0]**2 + 2*w[0]*(w[1]**3)\n",
    "\n",
    "    # Gradient\n",
    "    df0 = 2*w[0]+2*(w[1]**3)\n",
    "    df1 = 6*w[0]*(w[1]**2)\n",
    "    fgrad = np.array([df0, df1])\n",
    "    \n",
    "    return f, fgrad\n",
    "\n",
    "# Point to evaluate \n",
    "w = np.array([2,4])\n",
    "f, fgrad = feval(w)\n",
    "\n",
    "print('f     = %f' % f)\n",
    "print('fgrad = ' + str(fgrad))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "f     = 260.000000\n",
      "fgrad = [132 192]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 2:  Non-Linear Least Squares for an Exponential Model\n",
    "\n",
    "Consider an exponential model \n",
    "\n",
    "    yhat = a*exp(-b*x)\n",
    "    \n",
    "for parameters `w=[a,b]`.  Given training data `(x[i],y[i])` a natural loss function is given by\n",
    "\n",
    "    J(w) := \\sum_i (y[i] - yhat[i])**2,   yhat[i] = a*exp(-b*x[i])\n",
    "    \n",
    "The following code computes the the loss function `J(w)` and its gradient `dJ/dw`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def Jeval(w):\n",
    "    \n",
    "    # Unpack vector\n",
    "    a = w[0]\n",
    "    b = w[1]\n",
    "    \n",
    "    # Compute the loss function\n",
    "    yerr = y-a*np.exp(-b*x)\n",
    "    J = 0.5*np.sum(yerr**2)\n",
    "\n",
    "    # Compute the gradient\n",
    "    dJ_da = -np.sum( yerr*np.exp(-b*x))\n",
    "    dJ_db = np.sum( yerr*a*x*np.exp(-b*x))\n",
    "    Jgrad = np.array([dJ_da, dJ_db])\n",
    "    return J, Jgrad\n",
    "\n",
    "# Generate some random data\n",
    "ny = 100\n",
    "y = np.random.randn(ny)\n",
    "x = np.random.rand(ny)\n",
    "\n",
    "# Some arbitrary parameters \n",
    "# to compute the gradient at\n",
    "a = 1\n",
    "b = 2\n",
    "w = np.array([a,b])\n",
    "\n",
    "J, Jgrad = Jeval(w)\n",
    "print('Jgrad = ' + str(Jgrad))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jgrad = [27.4427924  -6.12391384]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 3:  Loss Function for a Log-Linear Model\n",
    "\n",
    "Now suppose we have a logarithmic linear model:\n",
    "\n",
    "    yhat = log(z),  z = w_0 + \\sum_j w_jx_j\n",
    "    \n",
    "If we have data `X, y`, the prediction on sample `i` is:\n",
    "\n",
    "    yhat_i = log(z_i),  z_i = w_0 + \\sum_j w_j*X_{ij}\n",
    " \n",
    "    \n",
    "Suppose we use MSE loss:\n",
    "\n",
    "    J(w) = \\sum_i (y_i - yhat_i )**2\n",
    "    \n",
    "To compute the components `dJ/dw_j`, first write `z = Aw` where `A=[1 X]`, the matrix with ones on the first column.\n",
    "Then, `dz_i/dw_j = A_{ij}`.  Also, `dyhat_i / dz_i = 1/z_i`, so with the multi-variable chain rule: \n",
    "\n",
    "    dJ/dw_j = \\sum_i dJ/dyhat_i * dyhat_i / dw_j \n",
    "            = \\sum_i (dJ/dyhat_i) * (dyhat_i / dz_i) * (dz_i / dw_j) = \n",
    "            = 2*\\sum_i (yhat_i - y_i) * 1/z_i* A_{ij}\n",
    "    \n",
    "We can implement the loss and gradient computation as follows:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def Jeval(w,X,y):\r\n",
    "\r\n",
    "    # Create matrix A=[1 X]\r\n",
    "    n = X.shape[0]\r\n",
    "    A = np.column_stack((np.ones(n), X))\r\n",
    "\r\n",
    "    # Compute function\r\n",
    "    z = A.dot(w)\r\n",
    "    yhat = np.log(z)\r\n",
    "    J = np.sum((y-yhat)**2)\r\n",
    "    \r\n",
    "    # Compute gradient\r\n",
    "    dJ_dz = 2*(yhat-y)/z\r\n",
    "    Jgrad = A.T.dot(dJ_dz)\r\n",
    "    \r\n",
    "    return J, Jgrad  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whenever you implement a gradient, you should always check the gradients.  Errors in the gradient are the number \n",
    "\n",
    "*  Take two points `w0` and `w1` close to one another\n",
    "*  Verify that `J(w1)-J(w0)` is close to `Jgrad(w0).dot(w1-w0)`. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Generate random positive data\r\n",
    "n = 100\r\n",
    "d = 5\r\n",
    "X = np.random.uniform(0,1,(n,d))\r\n",
    "w0 = np.random.uniform(0,1,(d+1,))\r\n",
    "y = np.random.uniform(0,2,(n,))\r\n",
    "\r\n",
    "# Compute function and gradient at point w0\r\n",
    "J0, Jgrad0 = Jeval(w0,X,y)\r\n",
    "\r\n",
    "# Take a small perturbation\r\n",
    "step = 1e-4\r\n",
    "w1 = w0 + step*np.random.normal(0,1,(d+1,))\r\n",
    "\r\n",
    "# Evaluate the function at perturbed point\r\n",
    "J1, Jgrad1 = Jeval(w1,X,y)\r\n",
    "\r\n",
    "dJ = J1-J0\r\n",
    "dJ_est = Jgrad0.dot(w1-w0)\r\n",
    "print('Actual difference:     %12.4e' % dJ)\r\n",
    "print('Estimated difference:  %12.4e' % dJ_est)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Actual difference:       2.9405e-03\n",
      "Estimated difference:    2.9398e-03\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 3:  A Function of a Matrix.\n",
    "\n",
    "Suppose `f(W) = a'*W*b`.  Then, `fgrad(W) = a*b.T`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def feval(W,a,b):\r\n",
    "    # Function\r\n",
    "    f = a.dot(W.dot(b))\r\n",
    "\r\n",
    "    # Gradient -- Use python broadcasting\r\n",
    "    fgrad = a[:,None]*b[None,:]\r\n",
    "    \r\n",
    "    return f, fgrad\r\n",
    "    \r\n",
    "# Some random data\r\n",
    "m = 4\r\n",
    "n = 3\r\n",
    "W = np.random.randn(m,n)\r\n",
    "a = np.random.randn(m)\r\n",
    "b = np.random.randn(n)\r\n",
    "\r\n",
    "f, fgrad = feval(W,a,b)\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## In-Class Exercise:  An Exponential Model\n",
    "\n",
    "Consider a model,\n",
    "\n",
    "    yhat = w[0]*exp(-w[1]*(x-w[2])**2/2)\n",
    "   \n",
    "where the parameter `w[2] > 0` is positive.\n",
    "\n",
    "Now, suppose that, given data `x` and `y`, we want to minimize the MSE loss function,\n",
    "\n",
    "    J = mean( (y[i] - yhat[i])**2 ) \n",
    "   \n",
    "Complete the following function to compute `J` and its gradient for parameters `w` and data (`x,y`)."
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def Jeval(w,X,y):\r\n",
    "    # TODO    \r\n",
    "    return J, Jgrad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the gradient:\n",
    "\n",
    "* Generate random `x` and `y` vectors with length `n=100`.\n",
    "* Compute `J0, Jgrad0`, the loss function and its gradient, at the `w0` value below\n",
    "* Compute some `w1`close to `w0`. \n",
    "* Compute `J1, Jgrad1`, the loss function and its gradient, at `w1`.\n",
    "* Check that `J1-J0` is approximately `Jgrad0.dot(w1-w0)`.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "w0 = np.array([0.4, 2, 2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "a = np.array([1,1])\r\n",
    "b = np.array([1,1])\r\n",
    "x = np.array([1,2,3,4])\r\n",
    "y = np.array([1,2,3,4])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "np.sum(a[None,:]*np.exp(-(x[:,None]-b[None,:])**2/2), axis=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2.        , 1.21306132, 0.27067057, 0.02221799])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "np.exp(-(x[:,None]-b[None,:])**2/2).dot(a)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2.        , 1.21306132, 0.27067057, 0.02221799])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "cb933b75c3113f96b7f71a67df95a9f5e58f40260f028aa6f0369a01be9c900d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}